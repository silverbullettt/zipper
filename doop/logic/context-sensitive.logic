/**
 * Generic context-sensitive pointer analysis
 *
 * @author Martin Bravenboer
 * @author Yannis Smaragdakis
 * @author George Kastrinis
 */

// There should be no need for this. It's a bad sign if there is.
//lang:compiler:disableError:NEGATION_RECURSION[]=true.

// Ignore verification of calculation in head
lang:compiler:warning:SPECIFIC_STARRED_EDGE_IN_SAFETY_GRAPH_CYCLE[] = false.

// This is a temporary hack. See near bottom for a proper definition of
// these predicates. Unfortunately derived predicates don't seem to
// be supported by the join optimizer currently.
#define ObjectShouldNotBeRefined(heap) \
    ((NegativeObjectFilter("true"), ObjectToRefine(heap)); \
    (!NegativeObjectFilter("true"), !ObjectToRefine(heap)))
#define ObjectShouldBeRefined(heap) \
    ((!NegativeObjectFilter("true"), ObjectToRefine(heap)); \
    (NegativeObjectFilter("true"), !ObjectToRefine(heap)))
#define SiteShouldNotBeRefined(invocation) \
    ((!NegativeSiteFilter("true"), !SiteToRefine(invocation)); \
    (NegativeSiteFilter("true"), SiteToRefine(invocation)))
#define SiteShouldBeRefined(invocation) \
    ((!NegativeSiteFilter("true"), SiteToRefine(invocation)); \
    (NegativeSiteFilter("true"), !SiteToRefine(invocation)))


/**
 * Heap allocation
 */

AssignNormalHeapAllocationSkolemOpt(?heap, ?ctx, ?var) ->
  HeapAllocationRef(?heap), Context(?ctx), VarRef(?var).

AssignNormalHeapAllocationSkolemOpt(?heap, ?ctx, ?var) <-
  AssignNormalHeapAllocation(?heap, ?var, ?inmethod),
  ReachableContext(?ctx, ?inmethod),
#ifdef RecordArrayMacro
  !ArrayType(HeapAllocation:Type[?heap]), 
#endif
  ObjectShouldNotBeRefined(?heap).

RecordMacro(?ctx, ?heap, ?hctx),
VarPointsTo(?hctx, ?heap, ?ctx, ?var) <-
  AssignNormalHeapAllocationSkolemOpt(?heap, ?ctx, ?var).

// There is scarcely any logical explanation, but breaking up the rule
// as above results in faster execution!
//RecordMacro(?ctx, ?heap, ?hctx),
//VarPointsTo(?hctx, ?heap, ?ctx, ?var) <-
//  AssignNormalHeapAllocation(?heap, ?var, ?inmethod),
//  ReachableContext(?ctx, ?inmethod),
//  ObjectShouldNotBeRefined(?heap).

#ifdef RecordArrayMacro
// This doesn't seem to pay off usually. So few analyses (e.g., 1-obj,
// which greatly benefits) define this macro and handle arrays
// specially.
AssignArrayHeapAllocationSkolemOpt(?heap, ?ctx, ?var) ->
  HeapAllocationRef(?heap), Context(?ctx), VarRef(?var).

AssignArrayHeapAllocationSkolemOpt(?heap, ?ctx, ?var) <-
  AssignNormalHeapAllocation(?heap, ?var, ?inmethod),
  ReachableContext(?ctx, ?inmethod),
  ArrayType(HeapAllocation:Type[?heap]),
  ObjectShouldNotBeRefined(?heap).

RecordArrayMacro(?ctx, ?heap, ?hctx),
VarPointsTo(?hctx, ?heap, ?ctx, ?var) <-
  AssignArrayHeapAllocationSkolemOpt(?heap, ?ctx, ?var).
#endif // def RecordArrayMacro

AssignAuxiliaryHeapAllocationSkolemOpt(?heap, ?ctx, ?var) ->
  HeapAllocationRef(?heap), Context(?ctx), VarRef(?var).

AssignAuxiliaryHeapAllocationSkolemOpt(?heap, ?ctx, ?var) <-
  AssignAuxiliaryHeapAllocation(?heap, ?var, ?inmethod),
  ReachableContext(?ctx, ?inmethod).

RecordMacro(?ctx, ?heap, ?hctx),
VarPointsTo(?hctx, ?heap, ?ctx, ?var) <-
  AssignAuxiliaryHeapAllocationSkolemOpt(?heap, ?ctx, ?var).

#ifdef RecordRefinedMacro
RecordRefinedMacro(?ctx, ?heap, ?hctx),
VarPointsTo(?hctx, ?heap, ?ctx, ?var) <-
  AssignNormalHeapAllocation(?heap, ?var, ?inmethod),
  ReachableContext(?ctx, ?inmethod),
  ObjectShouldBeRefined(?heap).
#endif

AssignContextInsensitiveHeapAllocationSkolemOpt(?heap, ?ctx, ?var) ->
  HeapAllocationRef(?heap), Context(?ctx), VarRef(?var).

AssignContextInsensitiveHeapAllocationSkolemOpt(?heap, ?ctx, ?var) <-
  AssignContextInsensitiveHeapAllocation(?heap, ?var, ?inmethod),
  ReachableContext(?ctx, ?inmethod).

RecordImmutableMacro(?ctx, ?heap, ?immCtx),
VarPointsTo(?immCtx, ?heap, ?ctx, ?var) <-
  AssignContextInsensitiveHeapAllocationSkolemOpt(?heap, ?ctx, ?var).

/**
 * Various assignments. Assign is a catch-all relation for
 * assignments not worth the effort to optimize specially.
 * Originally this used to be almost all assignments, but it
 * changed for performance reasons.
 */
VarPointsTo(?hctx, ?heap, ?toCtx, ?to) <-
  VarPointsTo(?hctx, ?heap, ?fromCtx, ?from),
  Assign(?type, ?toCtx, ?to, ?fromCtx, ?from),
  HeapAllocation:Type[?heap] = ?heaptype,
  AssignCompatible(?type, ?heaptype).


/**
 * Local assignments
Assign(?type, ?ctx, ?to, ?ctx, ?from) <-
  AssignLocal(?from, ?to, ?inmethod),
  ReachableContext(?ctx, ?inmethod),
  Var:Type[?to] = ?type.
*/

/*
// Assertion for sanity checking
VarPointsTo(_, ?heap, _, ?to) -> 
  AssignCompatible(Var:Type[?to], HeapAllocation:Type[?heap]).
*/

// No need to check if type compatible: check is done at original
// inputs to VarPointsTo
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  VarPointsTo(?hctx, ?heap, ?ctx, ?from),
  OptAssignLocal(?to, ?from).

 
OptAssignLocal(?to, ?from) -> VarRef(?to), VarRef(?from).
OptAssignLocal(?to, ?from) <-
  Reachable(?inmethod),
  AssignLocal(?from, ?to, ?inmethod).

/**
 * Cast assignments
 */
Assign(?type, ?ctx, ?to, ?ctx, ?from) <-
  AssignCast(?type, ?from, ?to, ?inmethod),
  ReachableContext(?ctx, ?inmethod).

/**
 * Load instance fields
 *
 * ctx would not hurt in ReachableLoadInstanceField, but it's not necessary.
 * TODO: might it help?
 */

/**
 * GKASTRINIS: Two ways to do field-based analysis.
 * Either based on the static type or on the dynamic one.
 */
#if defined FIELD_BASED_STATIC
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  OptLoadInstanceField2(?ctx, ?to, ?signature),
  InstanceFieldPointsTo(?hctx, ?heap, ?signature).

OptLoadInstanceField2(?ctx, ?to, ?signature) <-
  LoadInstanceField(_, ?signature, ?to, ?inmethod),
  ReachableContext(?ctx, ?inmethod).

#elif defined FIELD_BASED_DYNAMIC
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  LoadHeapInstanceField(?ctx, ?to, ?signature, _, ?baseheap), // any hcontext
  HeapAllocation:Type[?baseheap] = ?basetype,
  InstanceFieldPointsTo(?hctx, ?heap, ?signature, ?basetype).

#else
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  LoadHeapInstanceField(?ctx, ?to, ?signature, ?basehctx, ?baseheap),
  InstanceFieldPointsTo(?hctx, ?heap, ?signature, ?basehctx, ?baseheap).

#endif

LoadHeapInstanceField(?ctx, ?to, ?sig, ?basehctx, ?baseheap) <-
  ReachableLoadInstanceFieldBase(?base),
  OptLoadInstanceField(?to, ?sig, ?base),
  VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).

ReachableLoadInstanceFieldBase(?base) -> VarRef(?base).
ReachableLoadInstanceFieldBase(?base) <-
  LoadInstanceField(?base, _, _, ?inmethod),
  Reachable(?inmethod).

//
// TODO eliminate by reordering the input fact.
//
OptLoadInstanceField(?to, ?sig, ?base) <-
  LoadInstanceField(?base, ?sig, ?to, _).


/**
 * Store instance fields
 */
#if defined FIELD_BASED_STATIC
InstanceFieldPointsTo(?hctx, ?heap, ?signature) <-
  ReachableStoreInstanceFieldFrom(?from),
  OptStoreInstanceField2(?signature, ?from),
  VarPointsTo(?hctx, ?heap, _, ?from).

ReachableStoreInstanceFieldFrom(?from) -> VarRef(?from).
ReachableStoreInstanceFieldFrom(?from) <-
  StoreInstanceField(?from, _, _, ?inmethod),
  Reachable(?inmethod).

OptStoreInstanceField2(?signature, ?from) -> FieldSignatureRef(?signature), VarRef(?from).
OptStoreInstanceField2(?signature, ?from) <-
  StoreInstanceField(?from, _, ?signature, _).

#elif defined FIELD_BASED_DYNAMIC
InstanceFieldPointsTo(?hctx, ?heap, ?signature, ?basetype) <-
  StoreHeapInstanceField(?signature, _, ?baseheap, ?ctx, ?from), // any HContext
  HeapAllocation:Type[?baseheap] = ?basetype,
  VarPointsTo(?hctx, ?heap, ?ctx, ?from).

#else
InstanceFieldPointsTo(?hctx, ?heap, ?signature, ?basehctx, ?baseheap) <-
  StoreHeapInstanceField(?signature, ?basehctx, ?baseheap, ?ctx, ?from),
  VarPointsTo(?hctx, ?heap, ?ctx, ?from).
#endif

StoreHeapInstanceField(?signature, ?basehctx, ?baseheap, ?ctx, ?from) <-
  ReachableStoreInstanceFieldBase(?base),
  OptStoreInstanceField(?from, ?signature, ?base),
  VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).

ReachableStoreInstanceFieldBase(?base) -> VarRef(?base).
ReachableStoreInstanceFieldBase(?base) <-
  StoreInstanceField(_, ?base, _, ?inmethod),
  Reachable(?inmethod).

/**
 * TODO eliminate
 */
OptStoreInstanceField(?from, ?signature, ?base) <-
  StoreInstanceField(?from, ?base, ?signature, _).

/**
 * Load static fields
 */
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  OptLoadStaticField(?ctx, ?to, ?sig),
  StaticFieldPointsTo(?hctx, ?heap, ?sig).

OptLoadStaticField(?ctx, ?to, ?sig) <-
  LoadStaticField(?sig, ?to, ?inmethod),
  ReachableContext(?ctx, ?inmethod).

/**
 * Store static fields
 *
 * TODO: I don't think context actually matters. double check.
 */
StaticFieldPointsTo(?hctx, ?heap, ?signature) <-
  ReachableStoreStaticFieldFrom(?from),
  OptStoreStaticField(?signature, ?from),
  VarPointsTo(?hctx, ?heap, _, ?from).

OptStoreStaticField(?signature, ?from) <-
  StoreStaticField(?from, ?signature, _).

ReachableStoreStaticFieldFrom(?from) -> VarRef(?from).
ReachableStoreStaticFieldFrom(?from) <-
  StoreStaticField(?from, _, ?inmethod),
  Reachable(?inmethod).

/**
 * Load array index
 */
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  LoadHeapArrayIndex(?ctx, ?to, ?basehctx, ?baseheap),
  ArrayIndexPointsTo(?hctx, ?heap, ?basehctx, ?baseheap),
  Var:Type[?to] = ?type,
  HeapAllocation:Type[?baseheap] = ?baseheaptype,
  ComponentType[?baseheaptype] = ?basecomponenttype,
  AssignCompatible(?type, ?basecomponenttype).

/* YANNIS: used to be the much less precise:
  HeapAllocation:Type[?heap] = ?heaptype,
  AssignCompatible(?type, ?heaptype).
*/

LoadHeapArrayIndex(?ctx, ?to, ?basehctx, ?baseheap) <-
  ReachableLoadArrayIndexBase(?base),
  OptLoadArrayIndex(?to, ?base),
  VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).

OptLoadArrayIndex(?to, ?base) <-
  LoadArrayIndex(?base, ?to, _).

ReachableLoadArrayIndexBase(?base) -> VarRef(?base).
ReachableLoadArrayIndexBase(?base) <-
  LoadArrayIndex(?base, _, ?inmethod),
  Reachable(?inmethod).

/**
 * Store array index
 */

ArrayIndexPointsTo(?hctx, ?heap, ?basehctx, ?baseheap) <-
  StoreHeapArrayIndex(?basehctx, ?baseheap, ?ctx, ?from),
  VarPointsTo(?hctx, ?heap, ?ctx, ?from),
  HeapAllocation:Type[?heap] = ?heaptype,
  HeapAllocation:Type[?baseheap] = ?baseheaptype,
  ComponentType[?baseheaptype] = ?componenttype,
  AssignCompatible(?componenttype, ?heaptype).

StoreHeapArrayIndex(?basehctx, ?baseheap, ?ctx, ?from) <-
  ReachableStoreArrayIndexBase(?base),
  OptStoreArrayIndex(?from, ?base),
  VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).

ReachableStoreArrayIndexBase(?base) -> VarRef(?base).
ReachableStoreArrayIndexBase(?base) <-
  StoreArrayIndex(_, ?base, ?inmethod),
  Reachable(?inmethod).

OptStoreArrayIndex(?from, ?base) <-
  StoreArrayIndex(?from, ?base, _).

/**
 * Assignments for method invocations
 */


OptInterproceduralAssign(?toCtx, ?to, ?fromCtx, ?from) ->
  Context(?toCtx), VarRef(?to), Context(?fromCtx), VarRef(?from).

OptInterproceduralAssign(?calleeCtx, ?formal, ?callerCtx, ?actual)
  <-
  CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?method),
  FormalParam[?index, ?method] = ?formal,
  ActualParam[?index, ?invocation] = ?actual.

OptInterproceduralAssign(?callerCtx, ?local, ?calleeCtx, ?return)
  <-
  ReturnVar(?return, ?method),
  CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?method),
  AssignReturnValue[?invocation] = ?local.
  
VarPointsTo(?hctx, ?heap, ?toCtx, ?to) <-
  VarPointsTo(?hctx, ?heap, ?fromCtx, ?from),
  OptInterproceduralAssign(?toCtx, ?to, ?fromCtx, ?from).



/**
 * Static method invocations
 */

// Again, breaking up the rule minimizes skolem creation cost!
StaticMethodInvocationSkolemOpt(?callerCtx, ?invocation, ?tomethod) ->
  Context(?callerCtx), CallGraphEdgeSourceRef(?invocation), 
  MethodSignatureRef(?tomethod).

StaticMethodInvocationSkolemOpt(?callerCtx, ?invocation, ?tomethod) <-
  ReachableContext(?callerCtx, ?inmethod),
  StaticMethodInvocation(?invocation, ?signature, ?inmethod),
  MethodDeclaration[?signature] = ?tomethod.

#ifdef ENABLE_ZIPPER

MergeStaticMacro(?callerCtx, ?invocation, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod) <-
  StaticMethodInvocationSkolemOpt(?callerCtx, ?invocation, ?tomethod),
  ZipperContextSensitiveMethod(?tomethod).

MergeImmutableMacro(?immCtx),
CallGraphEdge(?callerCtx, ?invocation, ?immCtx, ?tomethod) <-
  StaticMethodInvocationSkolemOpt(?callerCtx, ?invocation, ?tomethod),
  !ZipperContextSensitiveMethod(?tomethod).

#else

MergeStaticMacro(?callerCtx, ?invocation, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod) <-
  StaticMethodInvocationSkolemOpt(?callerCtx, ?invocation, ?tomethod).

#endif // ENABLE_ZIPPER

/**
 * Virtual Method Invocation
 */

OptVirtualMethodInvocationBase(?invocation, ?base) -> 
  VarRef(?base), MethodInvocationRef(?invocation).

OptVirtualMethodInvocationBase(?invocation, ?base) <-
  Reachable(?inmethod),
  VirtualMethodInvocation:In(?invocation, ?inmethod),
  VirtualMethodInvocation:Base[?invocation] = ?base.


// This rule is the default logic for the majority of analyses

#ifndef OptimizeMergeMacro
// This is the "proper" form of the rule. We diverge from it
// only for reasons of optimization. Skolem object creation
// is currently very slow and we shouldn't invoke it for
// all the different combinations that will yield the same object.

MergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this)
 <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptVirtualMethodInvocationBase(?invocation, ?base),
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#else /* there is optimized behavior available */

// The optimization is as follows: the core analysis (this file) 
// first creates all the possible bindings that the Merge logic
// might need to create a new context. Then each individual analysis
// creates new context objects carefully by invoking the skolem
// functions as rarely as possible.
MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptVirtualMethodInvocationBase(?invocation, ?base).

#ifdef ENABLE_ZIPPER

CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  ZipperContextSensitiveMethod(?tomethod),
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

CallGraphEdge(?callerCtx, ?invocation, ?immCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?immCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  MergeImmutableMacro(?immCtx),
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  !ZipperContextSensitiveMethod(?tomethod),
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#else

// Finally, the step of the core analysis that should be creating the
// new objects is merely looking up the previously created context
// objects.
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx), 
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#endif // ENABLE_ZIPPER

#endif /* #ifndef OptimizeMergeMacro */


#ifdef MergeRefinedMacro
#ifndef OptimizeMergeRefinedMacro
// Default, unoptimized behavior
/**
 * This logic applies to refinement-based (or "adaptive") analyses. The 
 * analysis is first run with the filter predicate (SiteToRefine) empty, 
 * performing a low-precision but cheap computation. Then the external logic
 * runs delta rules to populate the filter predicate and re-runs the analysis.
 * (Note that the #ifdef just checks if the analysis has defined the
 * appropriate macro. This is not an input flag.)
 */
MergeRefinedMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptVirtualMethodInvocationBase(?invocation, ?base),
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldBeRefined(?invocation).

#else /* there is optimized behavior available */

MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptVirtualMethodInvocationBase(?invocation, ?base).

CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeRefinedMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx), 
  HeapAllocation:Type[?heap] = ?heaptype,
  VirtualMethodInvocation:SimpleName[?invocation] = ?simplename,
  VirtualMethodInvocation:Descriptor[?invocation] = ?descriptor,
  MethodLookup[?simplename, ?descriptor, ?heaptype] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldBeRefined(?invocation).
#endif /* #ifndef OptimizeMergeMacro */
#endif /* #ifdef MergeRefinedMacro */

/**
 * Special method invocations. Optimized much like virtual methods.
 */

OptSpecialMethodInvocationBase(?invocation, ?base) -> 
  VarRef(?base), MethodInvocationRef(?invocation).

OptSpecialMethodInvocationBase(?invocation, ?base) <-
  Reachable(?inmethod),
  SpecialMethodInvocation:In(?invocation, ?inmethod),
  SpecialMethodInvocation:Base[?invocation] = ?base.

#ifndef OptimizeMergeMacro
// Default, unoptimized behavior
MergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  VarPointsTo(?hctx, ?heap,  ?callerCtx, ?base),
  OptSpecialMethodInvocationBase(?invocation, ?base),
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#else /* there is optimized behavior available */

MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptSpecialMethodInvocationBase(?invocation, ?base).

#ifdef ENABLE_ZIPPER

CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  ZipperContextSensitiveMethod(?tomethod),
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

CallGraphEdge(?callerCtx, ?invocation, ?immCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?immCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  MergeImmutableMacro(?immCtx),
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  !ZipperContextSensitiveMethod(?tomethod),
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#else

CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx), 
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldNotBeRefined(?invocation).

#endif // ENABLE_ZIPPER
#endif /* #ifndef OptimizeMergeMacro */

#ifdef MergeRefinedMacro
#ifndef OptimizeMergeRefinedMacro
// Default, unoptimized behavior
MergeRefinedMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx),
CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptSpecialMethodInvocationBase(?invocation, ?base),
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldBeRefined(?invocation).

#else /* there is optimized behavior available */

MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap) <-
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?base),
  OptSpecialMethodInvocationBase(?invocation, ?base).

CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?tomethod),
VarPointsTo(?hctx, ?heap, ?calleeCtx, ?this) <-
  MergeBasisMacro(?callerCtx, ?invocation, ?hctx, ?heap),
  OptimizeMergeRefinedMacro(?callerCtx, ?invocation, ?hctx, ?heap, ?calleeCtx), 
  SpecialMethodInvocation:Signature[?invocation] = ?signature,
  MethodDeclaration[?signature] = ?tomethod,
  ThisVar[?tomethod] = ?this,
  SiteShouldBeRefined(?invocation).
#endif /* #ifndef OptimizeMergeMacro */
#endif /* #ifdef MergeRefinedMacro


/**
 * Reachable
 */
ReachableContext(?ctx, ?method) <-
  CallGraphEdge(_, _, ?ctx, ?method).

Reachable(?method) <-
  ReachableContext(_, ?method).

MethodInvocation:In(?invocation, ?inmethod) <-
  Reachable(?inmethod),
  (SpecialMethodInvocation:In(?invocation, ?inmethod);
	 VirtualMethodInvocation:In(?invocation, ?inmethod);
	 StaticMethodInvocation:In(?invocation, ?inmethod)).

/**
 * EXPERIMENTS ONLY below this point
 */

/**
 * Logic to decide whether to apply refined or regular bindings for
 * methods and objects
 */
// We want to allow predicates that express the *complement* of the set
// of objects to refine. We introduce derived-only temp predicates to
// avoid logic replication in the points-to rule itself.

/*
// Below is the proper way to write this but it's currently not well
// supported by the query optimizer so I have to resort to brute
// force (macro-)inlining.
ObjectShouldNotBeRefined(?heap) -> 
  HeapAllocationRef(?heap).
lang:derivationType[`ObjectShouldNotBeRefined] = "Derived".

ObjectShouldNotBeRefined(?heap) <-
  NegativeObjectFilter("true"), ObjectToRefine(?heap).

ObjectShouldNotBeRefined(?heap) <-
  !(NegativeObjectFilter("true")), !ObjectToRefine(?heap).

ObjectShouldBeRefined(?heap) -> 
  HeapAllocationRef(?heap).
lang:derivationType[`ObjectShouldBeRefined] = "Derived".

ObjectShouldBeRefined(?heap) <-
  !NegativeObjectFilter("true"), ObjectToRefine(?heap).

ObjectShouldBeRefined(?heap) <-
  NegativeObjectFilter("true"), !ObjectToRefine(?heap).

SiteShouldNotBeRefined(?invocation) -> 
  MethodInvocationRef(?invocation).
lang:derivationType[`SiteShouldNotBeRefined] = "Derived".

SiteShouldNotBeRefined(?invocation) <-
  !NegativeSiteFilter("true"), !SiteToRefine(?invocation).

SiteShouldNotBeRefined(?invocation) <-
  NegativeSiteFilter("true"), SiteToRefine(?invocation).

SiteShouldBeRefined(?invocation) -> 
  MethodInvocationRef(?invocation).
lang:derivationType[`SiteShouldBeRefined] = "Derived".

SiteShouldBeRefined(?invocation) <-
  !NegativeSiteFilter("true"), SiteToRefine(?invocation).

SiteShouldBeRefined(?invocation) <-
  NegativeSiteFilter("true"), !SiteToRefine(?invocation).
*/

/*
// YANNIS: It is tempting to think that the code below works better
//  than computing InstanceFieldPointsTo as an intermediate step.
//  It doesn't. Objects are fewer than vars. Always avoid var
//  cartesian products for efficiency.
VarPointsTo(?hctx, ?heap, ?toCtx, ?to) <-
  VarPointsTo(?hctx, ?heap, ?fromCtx, ?from),
  FlowsTo(?toCtx, ?to, ?fromCtx, ?from).

FlowsTo(?toCtx, ?to, ?fromCtx, ?from) <-
  StoreHeapInstanceField(?sig, ?basehctx, ?baseheap, ?fromCtx, ?from),
  LoadHeapInstanceField(?toCtx, ?to, ?sig, ?basehctx, ?baseheap).
*/

/*
// YANNIS: There's hardly any reason why the code below might work
// better than regular interprocedural assignments (minor exception:
// for return vars, there are methods that have multiple, so some
// benefit might exist). But it was tempting, since interprocedural
// assignments are such a bottleneck. This code doesn't pay off though.
OptActualParam(?index, ?invocation, ?actual) <-
  ActualParam[?index, ?invocation] = ?actual.

OptInvocationWithParam(?index, ?calleeCtx, ?method, ?callerCtx, ?actual) <-
  ActualParam[?index, ?invocation] = ?actual,
  CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?method).

MethodArgPointsTo(?hctx, ?heap, ?index, ?calleeCtx, ?method) <-
  OptInvocationWithParam(?index, ?calleeCtx, ?method, ?callerCtx, ?actual),
  VarPointsTo(?hctx, ?heap, ?callerCtx, ?actual).

VarPointsTo(?hctx, ?heap, ?calleeCtx, ?formal) <-
  FormalParam[?index, ?method] = ?formal,
  MethodArgPointsTo(?hctx, ?heap, ?index, ?calleeCtx, ?method).

OptReturnVar(?method, ?return) <-
  ReturnVar(?return, ?method).

ReturnVarPointsTo(?hctx, ?heap, ?calleeCtx, ?method) <-
  OptReturnVar(?method, ?return),
  VarPointsTo(?hctx, ?heap, ?calleeCtx, ?return).

VarPointsTo(?hctx, ?heap, ?callerCtx, ?local) <-
  CallGraphEdge(?callerCtx, ?invocation, ?calleeCtx, ?method),
  AssignReturnValue[?invocation] = ?local,
  ReturnVarPointsTo(?hctx, ?heap, ?calleeCtx, ?method).
*/
/* YANNIS
VarPointsTo(?hctx, ?heap, ?ctx, ?to) <-
  LoadHeapInstanceField(?ctx, ?to, ?signature, ?basehctx, ?baseheap),
  InstanceFieldPointsTo(?hctx, ?heap, ?signature, ?basehctx, ?baseheap).

LoadHeapInstanceField(?ctx, ?to, ?sig, ?basehctx, ?baseheap) <-
  ReachableLoadInstanceField(?to, ?sig, ?base),
  VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).
#endif

ReachableLoadInstanceField(?to, ?sig, ?base) -> 
  VarRef(?to), FieldSignatureRef(?sig), VarRef(?base).
ReachableLoadInstanceField(?to, ?sig, ?base) <-
  LoadInstanceField(?base, ?sig, ?to, ?inmethod),
  Reachable(?inmethod).
*/

/*
StoreHeapArrayIndex(?basehctx, ?baseheap, ?ctx, ?from) <-
	ReachableStoreArrayIndex(?from, ?base),
	VarPointsTo(?basehctx, ?baseheap, ?ctx, ?base).

ReachableStoreArrayIndex(?from, ?base) <-
	StoreArrayIndex(?from, ?base, ?inmethod),
	Reachable(?inmethod).
*/
